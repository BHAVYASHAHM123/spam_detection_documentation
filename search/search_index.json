{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spam Detection Deep Learning Technology (NLP)","text":"<p>Problem</p> <p>Email spam has become a serious issue in recent years, and as the number of internet users grows, so does the number of spam emails. They are being used for unlawful and immoral activities, such as phishing and fraud. Sending harmful links via spam emails, which can destroy our system as well as get access to yours. Spammers can easily create a fake profile and email account, and in their spam emails, they appear to be a real person. These spammers target those who are unaware of the scams.</p>"},{"location":"architect/","title":"Approach","text":"<p>Approach</p> <p>Machine learning methods of recent are being used to successfully detect and filter spam emails. Some Methods are Content Based Filtering Technique, Case Base Spam Filtering Method, Heuristic or Rule Based Spam Filtering Technique or Previous Likeness Based Spam Filtering Technique.</p> Diagram <pre><code>graph LR\n\nA[Spam] ---&gt; C{Feature Extraction};\nB[Ham] ---&gt; C{Feature Extraction};\nC ---&gt;|Naive Bayes| D[Score Based Spam Detection]\nD ---&gt; E{It's Spam Message};\nD ---&gt; E{It's Ham Message};</code></pre>"},{"location":"code/","title":"Implementation","text":""},{"location":"code/#dataset","title":"Dataset","text":"<p>For dataset visit kaggle.com.</p> Importing the important libraries <pre><code>import numpy as np\nimport pandas as pd\n</code></pre> Import Dataset (jupiter or colab) <pre><code>df = pd.read_csv('spam.csv', encoding='latin-1')\n</code></pre> Displaying some data from the dataset <pre><code>df.sample(5)\n</code></pre> <p>Here we can see that we have five column .</p> Checking the shape of the dataset <pre><code>df.shape\n</code></pre>"},{"location":"code/#1-data-cleaning","title":"(1.) DATA CLEANING","text":"Checking the information of the dataset <pre><code>df.info()\n</code></pre> <p>here we can see that in </p> <p><code>Unnamed: 2  50 non-null     object</code> <code>3   Unnamed: 3  12 non-null     object</code> <code>4   Unnamed: 4  6 non-null      object</code></p> <p>have many null values  So, we have to drop this columns</p> Droping the last three columns of the dataset <pre><code>df.drop(columns = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\ndf.head(5)\n</code></pre> <p>here, <code>inplace=True</code> means it will permanently drop the column from the dataset</p> Replacing the column name with meaningfull words <pre><code>df.rename(columns = {'v1' : 'target', 'v2' : 'text'}, inplace=True)\n</code></pre> converting the target column data from text to number using label encoder <pre><code>from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndf['target'] = encoder.fit_transform(df['target'])\n</code></pre> <p>Label_encoder</p> <p>converting the labels into a numeric form so as to convert them into the machine-readable form</p> <p>Fit_transform</p> <p>Converting the training data from unit to unitless</p> Checking the null values <pre><code>df.isnull().sum()\n</code></pre> Checking dublicate data <pre><code>df.duplicated().sum()\n</code></pre> removing the duplicate data <pre><code>df.drop_duplicates(keep = 'first')\n</code></pre>"},{"location":"code/#2-eda-exploratory-data-analysis","title":"(2.) EDA (Exploratory Data Analysis)","text":"counting the spam and ham data present in the database <pre><code>df['target'].value_counts()\n</code></pre> Plotting the graph <pre><code>import matplotlib.pyplot as plt\nplt.pie(df['target'].value_counts(), labels = ['ham', 'spam'], autopct = '%0.2f')\n</code></pre> <p>Here, we can see that the <code>data is imbalanced</code></p> <p>More that <code>85% of data</code> is <code>Ham (Not-Spam)</code>  and just <code>15% of data</code> is <code>Spam</code></p> checking the character length of the text data <pre><code>df['num_characters'] = df['text'].apply(len)\ndf['num_characters']\n</code></pre> checking the number of words of text data <pre><code>df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))\ndf['num_words']\ndf.head()\n</code></pre> checking the number of sentences of text data <pre><code>df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\ndf['num_sentences']\ndf.head()\n</code></pre> printing the description of the data <pre><code>df[['num_characters', 'num_words', 'num_sentences']].describe()\n</code></pre> <p>Here, in this section we can see the mean, std deviation, min, max, 25%, 50%, 75% of data with respect to ['num_characters', 'num_words', 'num_sentences']</p> describing the data on basis of ham <pre><code>df[df['target'] == 0][['num_characters', 'num_words', 'num_sentences']].describe()\n</code></pre> describing the data on basis of spam <pre><code>df[df['target'] == 1][['num_characters', 'num_words', 'num_sentences']].describe()\n</code></pre> Checking and printing histogram how much number of charactes are there in ham and in spam data <pre><code>import seaborn as sns\nplt.figure(figsize = (12, 5))\nsns.histplot(df[df['target'] == 0]['num_characters'])\nsns.histplot(df[df['target'] == 1]['num_characters'], color = 'red')\n</code></pre> <p>here it will show the clear hist graph of number_of_character present in spam and ham we can see that number of character count is more in ham and very less number of character count in ham.</p> Checking and printing histogram how much number of words are there in ham and in spam data <pre><code>plt.figure(figsize = (12, 5))\nsns.histplot(df[df['target'] == 0]['num_words'])\nsns.histplot(df[df['target'] == 1]['num_words'], color = 'red')\n</code></pre> <p>here it will show the clear hist graph of number_of_words present in spam and ham we can see that words of character count is more in ham and very less number of words count in ham.</p> Checking and printing histogramhow much number of sentences are there in ham and in spam data <pre><code>plt.figure(figsize = (12, 5))\nsns.histplot(df[df['target'] == 0]['num_sentences'])\nsns.histplot(df[df['target'] == 1]['num_sentences'], color = 'red')\n</code></pre> <p>here it will show the clear hist graph of number_of_sentence present in spam and ham we can see that words of sentence count is more in ham and very less number of sentence count in ham.</p> Printing the pair plot graph b/w num_sentences, num_words and num_characters <pre><code>sns.pairplot(df, hue = 'target')\n</code></pre> Printing the heat map for checking the corelation <pre><code>sns.heatmap(df.corr(), annot = True)\n</code></pre>"},{"location":"code/#3-data-preprocessing","title":"(3.) Data Preprocessing","text":"<p>Lower_case</p> <p>Converting all the text into lower case ( helps in the process of preprocessing and in later stages in the NLP application, when you are doing parsing.)</p> <p>Tokenization</p> <p>The process of  splitting a string, text into a list of tokens.</p> <p>Removing</p> <p>@ # $ % etc,.</p> <p>Removing<p>english stop words eg (is, at, was, etc,.)</p> </p> <p>Stemming<p>bringing the word at its root form eg : (dancing, danced, dancer = <code>(dance is the root word )</code> )</p> </p> <p>Punctuation<p>!\"#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~</p> </p> importing stemming <pre><code>from nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\nps.stem('hugging')\n</code></pre> importing stop words <pre><code>from nltk.corpus import stopwords\nstopwords.words('english')\n</code></pre> importing punctuation strings <pre><code>import string\nstring.punctuation\n</code></pre> Creating transforming text function <pre><code>def transform_text(text):\n#converting to lower case (eg: (I am Bhavya) ---&gt; (i am bhavya) )\ntext = text.lower()\n#creating the tokens for single-single words in the list format (eg:['i', 'am', 'bhavya'])\ntext = nltk.word_tokenize(text)\n# removing the special character using (is alpha numeric keyword) (eg: (i @ bhavya) ---&gt; (i bhavya))\ny = []\nfor i in text:\nif i.isalnum():\ny.append(i)\ntext = y[:]\ny.clear()\n# removing english stop words and punctuation\nfor i in text:\nif i not in stopwords.words('english') and i not in string.punctuation:\ny.append(i)\ntext = y[:]\ny.clear()\n#applying stemming (bringing to its root form  eg : dancing, danced, dancer = (dance is the root word ))\nfor i in text:\ny.append(ps.stem(i))\n# joining the return empty string with y\nreturn \" \".join(y)\n</code></pre> Applying the transformed text function to the text column and storing in the newly created column <pre><code>df['transform_text'] = df['text'].apply(transform_text)\n</code></pre>"},{"location":"code/#4-creating-word-cloud-of-the-data","title":"(4.) Creating word cloud of the data","text":"Word Cloud Intasll <pre><code>!pip install wordcloud #installing wordcloud\n</code></pre> \"Displaying the higest number of words used in the spam using word cloud <pre><code># Displaying the higest number of words used in the spam using word cloud\nfrom wordcloud import WordCloud\nwc = WordCloud(width = 1500, height = 1500, min_font_size = 20, background_color = 'black')    \nspam_wc = wc.generate(df[df['target'] == 1]['transform_text'].str.cat(sep = ' '))\nplt.figure(figsize = (12, 6))\nplt.imshow(spam_wc)\n</code></pre> Displaying the higest number of words used in the ham using word cloud <pre><code># Displaying the higest number of words used in the ham using word cloud\nplt.figure(figsize = (12, 6))\nwc = WordCloud(width = 1500, height = 1500, min_font_size = 20, background_color = 'white')\nham_wc = wc.generate(df[df['target'] == 0]['transform_text'].str.cat(sep = ' '))\nplt.imshow(ham_wc)\n</code></pre> <p>Word<p>Higest number of words used in the message.</p> </p> checking the length and diplaying the top 30 common words used in spam <pre><code>spam_corpus = []\nfor msg in df[df['target'] == 1]['transform_text'].tolist():\nfor word in msg.split():\nspam_corpus.append(word)\nlen(spam_corpus)\n</code></pre> <pre><code>from collections import Counter\nCounter(spam_corpus).most_common(30)\n</code></pre> Creating the tick graph of the most common word in the spam <pre><code>from collections import Counter\nsns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()\n</code></pre> checking the length and diplaying the top 30 common words used in ham <pre><code>ham_corpus = []\nfor msg in df[df['target'] == 0]['transform_text'].tolist():\nfor word in msg.split():\nham_corpus.append(word)\nlen(ham_corpus)\n</code></pre> Creating the tick graph of the most common word in the ham <pre><code>```python\nfrom collections import Counter\nsns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\nplt.xticks(rotation='vertical')\nplt.show()\n```\n</code></pre>"},{"location":"code/#5-model-building","title":"(5.) Model Building","text":"Extracting the features from text using count vectorizer, tfidf vectorizer <p><pre><code>from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\ncv = CountVectorizer()\ntfidf = TfidfVectorizer(max_features=3000)\n</code></pre> <pre><code>X = tfidf.fit_transform(df['transform_text']).toarray()\nX.shape\n</code></pre></p> <pre><code>y = df['target'].values\ny.shape\n</code></pre> <p>Countvectorizer<p>means breaking down a sentence or any text into words by performing preprocessing tasks like converting all words to lowercase, thus removing special characters. tell unique value.</p> </p> <p>Tf-idf<p>It is used by search engines to better understand the content that is undervalued. For example, when you search for \u201cCoke\u201d on Google, Google may use TF-IDF to figure out if a page titled \u201cCOKE\u201d is about: a) Coca-Cola.</p> </p> spliting the data into test and train <pre><code>from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n</code></pre> importing the model libraries <pre><code>from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n</code></pre> <p>Easy<p>naive bayes and it's types.</p> </p> Object Creation <pre><code># creating the object Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes\ngnb = GaussianNB()\nmnb = MultinomialNB()\nbnb = BernoulliNB()\n</code></pre> model training code of Gaussian Naive Bayes <pre><code># model training code of Gaussian Naive Bayes\ngnb.fit(X_train,y_train)\ny_pred1 = gnb.predict(X_test)\nprint(accuracy_score(y_test,y_pred1))\nprint(confusion_matrix(y_test,y_pred1))\nprint(precision_score(y_test,y_pred1))\n</code></pre> model training code of Multinomial Naive Bayes <pre><code># model training code of Multinomial Naive Bayes\nmnb.fit(X_train,y_train)\ny_pred2 = mnb.predict(X_test)\nprint(accuracy_score(y_test,y_pred2))\nprint(confusion_matrix(y_test,y_pred2))\nprint(precision_score(y_test,y_pred2))\n</code></pre> model training code of Bernoulli Naive Bayes <pre><code># model training code of Bernoulli Naive Bayes\nbnb.fit(X_train,y_train)\ny_pred3 = bnb.predict(X_test)\nprint(accuracy_score(y_test,y_pred3))\nprint(confusion_matrix(y_test,y_pred3))\nprint(precision_score(y_test,y_pred3))\n</code></pre> Storing the model in the Pickle file <pre><code># Creating the Pickle file of the model\nimport pickle\npickle.dump(tfidf,open('vectorizer.pkl','wb'))\npickle.dump(mnb,open('model.pkl','wb'))\n</code></pre>"},{"location":"naive_bayes/","title":"Naive Bayes","text":"<p>Naive</p> <p>Naive Bayes Suppose we have a dataset of weather conditions and corresponding target variable \"Play\". So using this dataset we need to decide that whether we should play or not on a particular day according to the weather conditions.</p> <ol> <li> <p>Convert the given dataset into frequency tables.</p> </li> <li> <p>Generate Likelihood table by finding the probabilities of given features.</p> </li> <li> <p>Now, use Bayes theorem to calculate the posterior probability.</p> </li> </ol> <p>Problem<p>If the weather is sunny, then the Player should play or not?</p> </p> <p>Solution<p>To solve this, first consider the below dataset:</p> </p> <p> </p> <p>Advantages</p> <p>Naive Bayes is one of the fast and easy ML algorithms to predict a class of datasets. It can be used for Binary as well as Multi-class Classifications. It performs well in Multi-class predictions as compared to the other Algorithms. It is the most popular choice for text classification problems. Disadvantages of Na\u00efve Bayes Classifier: Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.</p>"},{"location":"naive_bayes/#applications-of-naive-bayes-classifier","title":"Applications of Na\u00efve Bayes Classifier:","text":"<p><code>It is used for Credit Scoring.</code></p> <p><code>It is used in medical data classification.</code></p> <p><code>It can be used in real-time predictions because Na\u00efve Bayes Classifier is an eager learner.</code></p> <p><code>It is used in Text classification such as Spam filtering and Sentiment analysis.</code></p>"},{"location":"naive_bayes/#types-of-naive-bayes-model","title":"Types of Naive Bayes Model:","text":"<p>There are three types of Naive Bayes Model, which are given below:</p> <p>Gaussian</p> <p>The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.</p> <p>Multinomial</p> <p>The Multinomial Na\u00efve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors.</p> <p>Bernoulli</p> <p>The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is <code>yes or no</code> , <code>true or false</code> . This model is also famous for document classification tasks.</p> <p>Content<p>javatpoint</p> </p>"},{"location":"result/","title":"Result","text":"<p>For Demo Video of Spam Detection .</p> <p>Result</p> <p>The system should be able to easily classify between real and fake emails.You have to build a robust antispam filter solution that should identify fake and real emails.`</p> <p></p>"}]}